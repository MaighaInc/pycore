{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "29ba3e4c",
      "metadata": {},
      "source": [
        "# üìô P2.2.4.1 ‚Äì Reinforcement Learning\n",
        "\n",
        "## Topic: Reinforcement Learning ‚Äî Agents, Rewards & Decisions\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3da3610f",
      "metadata": {},
      "source": [
        "> üìç In **ML Basics & Terminologies** you saw the **three types** of ML (supervised, unsupervised, reinforcement).\n",
        "\n",
        "This section focuses only on **reinforcement learning** ‚Äî where an **agent** interacts with an **environment**, takes **actions**, and gets **rewards**.\n",
        "\n",
        "## üéØ Learning Objectives\n",
        "\n",
        "By the end of this notebook, you will be able to:\n",
        "\n",
        "- Define **reinforcement learning** in simple terms (agent ‚Üí actions ‚Üí rewards ‚Üí learn a policy)\n",
        "- Understand how RL differs from **supervised** and **unsupervised** learning\n",
        "- Recognize common RL terms: **agent, environment, state, action, reward, episode**\n",
        "- Know **major RL algorithm families** at a high level (Q-Learning, DQN, Policy Gradient, Actor-Critic)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "edca1bf9",
      "metadata": {},
      "source": [
        "## 1Ô∏è‚É£ Reinforcement Learning üéÆ\n",
        "\n",
        "**What is it?**\n",
        "\n",
        "Reinforcement learning is a type of machine learning where an **agent** learns to make decisions by **interacting** with an **environment** and getting **rewards** (or penalties).\n",
        "\n",
        "- The agent observes a **state** (what's happening now)\n",
        "- Chooses an **action**\n",
        "- Receives a **reward** (good/bad)\n",
        "- Moves to a **new state**\n",
        "\n",
        "Over many **episodes** (tries), it learns a **policy** ‚Äî a way of choosing actions to **maximize total reward**.\n",
        "\n",
        "**Where do we use it? (examples)**\n",
        "- Game playing (Chess, Go, Atari)\n",
        "- Robotics (robot navigation, control)\n",
        "- Recommendation / ad placement\n",
        "- Self-driving or lane-keeping controllers"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1df830e8",
      "metadata": {},
      "source": [
        "### üîô How RL differs from supervised & unsupervised\n",
        "\n",
        "- **Supervised learning:** We get **input + correct output** (X, y). The goal is to learn a mapping from X ‚Üí y.\n",
        "- **Unsupervised learning:** We get **only X** (no labels). The goal is to find **structure** (clusters, lower dimensions).\n",
        "- **Reinforcement learning:** We get **states, actions, rewards over time**. There is **no correct action given** ‚Äî only **feedback** (reward). The goal is to learn **how to act** to maximize long-term reward."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2a598cdc",
      "metadata": {},
      "source": [
        "## üîÄ Types of problems we solve in reinforcement learning\n",
        "\n",
        "In reinforcement learning we mainly solve **sequential decision-making** problems:\n",
        "\n",
        "1Ô∏è‚É£ **Game / control problems** ‚Äî agent chooses moves or control signals step by step (e.g. games, robot control).  \n",
        "2Ô∏è‚É£ **Recommendation / interaction problems** ‚Äî agent chooses which content or action to show over time (e.g. ads, recommendations, pricing)."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dad584bb",
      "metadata": {},
      "source": [
        "üìã **Quick RL view:**\n",
        "\n",
        "| Concept | RL term | One-line meaning |\n",
        "|--------|---------|------------------|\n",
        "| \"Who\" is acting? | **Agent** | The learner/decision-maker. |\n",
        "| Situation now | **State** | What the agent observes about the world. |\n",
        "| Choice | **Action** | What the agent can do now. |\n",
        "| Feedback | **Reward** | Number telling how good/bad that step was. |\n",
        "| Experience | **Episode** | One run from start to finish. |\n",
        "| Goal | **Policy** | Strategy for choosing actions in each state. |"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8913b128",
      "metadata": {},
      "source": [
        "## üìù Key Takeaways\n",
        "\n",
        "- **Reinforcement learning** = agent + environment + states, actions, rewards over time ‚Üí learn a **policy** to maximize long-term reward.\n",
        "- RL is different from supervised (X, y) and unsupervised (only X): there is **no correct action**, only **feedback** (reward).\n",
        "- Common RL terms: **agent, environment, state, action, reward, episode, policy** ‚Äî you saw them at a high level here.\n",
        "- Major RL algorithm families: **value-based** (Q-Learning, DQN), **policy-based** (Policy Gradient), and **Actor-Critic** (both together). You don't need details yet ‚Äî just the map."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
